{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 31)\n",
      "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0          2        17.99         10.38           122.8     1001.0   \n",
      "1          2        20.57         17.77           132.9     1326.0   \n",
      "2          2        19.69         21.25           130.0     1203.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "\n",
      "   symmetry_mean           ...             radius_worst  texture_worst  \\\n",
      "0         0.2419           ...                    25.38          17.33   \n",
      "1         0.1812           ...                    24.99          23.41   \n",
      "2         0.2069           ...                    23.57          25.53   \n",
      "\n",
      "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "0            184.6      2019.0            0.1622             0.6656   \n",
      "1            158.8      1956.0            0.1238             0.1866   \n",
      "2            152.5      1709.0            0.1444             0.4245   \n",
      "\n",
      "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0           0.7119                0.2654          0.4601   \n",
      "1           0.2416                0.1860          0.2750   \n",
      "2           0.4504                0.2430          0.3613   \n",
      "\n",
      "   fractal_dimension_worst  \n",
      "0                  0.11890  \n",
      "1                  0.08902  \n",
      "2                  0.08758  \n",
      "\n",
      "[3 rows x 31 columns]\n",
      "    0          1          2          3            4         5         6   \\\n",
      "0  1.0  15.622391  14.998648  70.511926   459.704247  0.076438  0.061816   \n",
      "1  2.0  11.244005  16.037444  89.678549   763.503960  0.104511  0.058981   \n",
      "2  2.0  11.386893  25.582710  66.905229  1269.686261  0.086103  0.197375   \n",
      "\n",
      "         7         8         9     ...            21         22          23  \\\n",
      "0  0.064378  0.030902  0.178959    ...     19.463507  20.126530   70.661804   \n",
      "1  0.090061  0.018774  0.171779    ...     21.657198  17.025713  118.700621   \n",
      "2  0.226351  0.005350  0.212174    ...     11.943945  27.388279   85.521669   \n",
      "\n",
      "           24        25        26        27        28        29        30  \n",
      "0  951.220909  0.127025  0.204443  0.241407  0.034524  0.213047  0.089422  \n",
      "1  354.003288  0.133202  0.293880  0.193965  0.151199  0.300232  0.098450  \n",
      "2  676.978922  0.135151  0.428027  0.242981  0.022757  0.249087  0.118132  \n",
      "\n",
      "[3 rows x 31 columns]\n",
      "(400, 31)\n"
     ]
    }
   ],
   "source": [
    "real_data = pd.read_csv('data/breast_cancer.csv',sep=',', index_col=False,\n",
    "        na_values=['NaN', 'nan', 'NULL', 'null'], low_memory=False)\n",
    "real_data.drop(real_data.columns[0], axis=1, inplace=True)\n",
    "real_data.drop(real_data.columns[len(real_data.columns)-1], axis=1, inplace=True)\n",
    "real_data.fillna(0)\n",
    "d = dict([(y,x+1) for x,y in enumerate(sorted(set(real_data['diagnosis'])))])\n",
    "real_data['diagnosis']=[d[x] for x in real_data['diagnosis']]\n",
    "fake_data = pd.read_csv('experiments/cancer_synthetic.csv',header=-1)\n",
    "fake_data[0] = fake_data[0].round(0)\n",
    "fake_data = fake_data.iloc[:400,:]\n",
    "print(real_data.shape)\n",
    "print(real_data.head(3))\n",
    "print(fake_data.head(3))\n",
    "print(fake_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 31)\n",
      "(169, 30)\n"
     ]
    }
   ],
   "source": [
    "n_train = 400\n",
    "n_test = len(real_data)-n_train\n",
    "#train test split\n",
    "real_train = real_data.iloc[:n_train,:]\n",
    "test_set = real_data.iloc[n_train:,:]\n",
    "test_set_y = test_set[['diagnosis']]\n",
    "test_set_x = test_set.drop(test_set.columns[0], axis=1, inplace=False)\n",
    "print(real_train.shape)\n",
    "print(test_set_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = real_train[[\"diagnosis\"]].values\n",
    "x_real = real_train.drop(real_train.columns[0], axis=1, inplace=False)\n",
    "y_fake = fake_data[0].values\n",
    "x_fake = fake_data.drop(fake_data.columns[0], axis=1, inplace=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867370073247\n",
      "0.770068027211\n",
      "[2 1 1 1 1 1 2 1 2 1 2 1 1 2 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 2 2 1 2 1\n",
      " 1 1 1 1 2 1 1 2 1 2 1 2 2 1 2 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 1 1 1 1\n",
      " 1 1 2 1 1 2 1 1 1 1 2 1 2 2 1 2 1 2 2 1 1 1 1 1 2 2 1 2 1 2 2 2 1 1 1 2 1\n",
      " 1 2 2 2 1 2 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 1 1 1 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 2 2 2 2 2 1]\n",
      "[ 2.  1.  1.  1.  1.  1.  2.  1.  2.  1.  1.  1.  1.  2.  2.  1.  1.  2.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  2.  2.  2.  1.\n",
      "  1.  1.  1.  1.  1.  2.  1.  1.  1.  2.  2.  2.  2.  2.  1.  2.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  2.  2.  1.  1.  1.  1.  1.  1.  2.  1.  1.  2.\n",
      "  2.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  2.  1.  2.  2.  1.  1.\n",
      "  1.  2.  2.  1.  1.  2.  1.  1.  1.  2.  2.  2.  1.  1.  1.  1.  1.  1.\n",
      "  2.  2.  1.  2.  1.  2.  2.  1.  2.  1.  1.  1.  1.  2.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  2.  1.  2.  1.  2.  1.  2.  1.  1.  2.  1.\n",
      "  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  1.  1.\n",
      "  2.  2.  2.  2.  2.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "model_real = tree.DecisionTreeClassifier(max_depth=20)\n",
    "model_real.fit(x_real, y_real)\n",
    "model_fake = tree.DecisionTreeClassifier(max_depth=20)\n",
    "model_fake.fit(x_fake, y_fake)\n",
    "pred_real = model_real.predict(test_set_x)\n",
    "pred_fake = model_fake.predict(test_set_x)\n",
    "print(f1_score(test_set_y, pred_real, average='macro'))\n",
    "print(f1_score(test_set_y, pred_fake, average='macro'))\n",
    "print(pred_real)\n",
    "print(pred_fake)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=500, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "[2 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1\n",
      " 1 1 1 1 2 1 1 2 1 2 1 1 2 1 2 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 1 1 1 2 1\n",
      " 1 1 2 1 1 1 1 2 1 1 1 1 1 2 1 2 1 2 2 1 1 1 1 1 2 2 1 2 1 2 1 1 1 1 1 1 1\n",
      " 1 2 2 2 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 1]\n",
      "[ 1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.\n",
      "  1.  2.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  2.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  2.  1.  1.  1.  1.]\n",
      "0.9091442256\n",
      "0.554027777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "x_real_n = normalize(x_real)\n",
    "x_fake_n = normalize(x_fake)\n",
    "model_real = svm.SVC(C=500)\n",
    "model_real.fit(x_real_n,y_real)\n",
    "print(model_real)\n",
    "model_fake = svm.SVC(C=500)\n",
    "model_fake.fit(x_fake_n,y_fake)\n",
    "test_set_n = normalize(test_set_x)\n",
    "pred_real = model_real.predict(test_set_n)\n",
    "\n",
    "pred_fake = model_fake.predict(test_set_n)\n",
    "print(pred_real)\n",
    "print(pred_fake)\n",
    "print(f1_score(test_set_y, pred_real, average='macro'))\n",
    "print(f1_score(test_set_y, pred_fake, average='macro'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 1 1 1 1 2 1 2 1 1 2 2 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 2 1 2 1\n",
      " 1 1 1 2 2 1 1 2 1 2 1 2 2 1 2 1 1 1 2 2 1 1 1 2 2 1 1 1 2 1 1 2 1 1 2 1 1\n",
      " 1 1 2 1 1 1 1 2 1 1 1 1 1 2 1 2 2 2 2 1 1 1 1 1 2 2 1 2 1 2 1 1 1 1 1 2 1\n",
      " 1 2 1 2 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 2 2 1 1 1 1 1\n",
      " 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2]\n",
      "[ 2.  1.  1.  1.  1.  1.  2.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  2.\n",
      "  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  2.  1.  2.  1.  2.\n",
      "  1.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.\n",
      "  2.  1.  1.  1.  1.  1.  1.  2.  1.  1.  1.  1.  2.  1.  1.  2.  1.  2.\n",
      "  1.  2.  2.  1.  1.  1.  1.  1.  2.  2.  2.  1.  1.  2.  1.  1.  1.  1.\n",
      "  2.  1.  1.  2.  1.  2.  1.  1.  2.  2.  1.  1.  1.  2.  1.  1.  1.  1.\n",
      "  1.  1.  2.  1.  1.  1.  1.  2.  1.  2.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  2.  2.  2.  1.  2.  1.]\n",
      "0.859073952162\n",
      "0.79630373644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model_real = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(3, 10), random_state=1).fit(x_real_n,y_real)\n",
    "model_fake = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(3, 10), random_state=1).fit(x_fake_n,y_fake)\n",
    "pred_real = model_real.predict(test_set_n)\n",
    "\n",
    "pred_fake = model_fake.predict(test_set_n)\n",
    "print(pred_real)\n",
    "print(pred_fake)\n",
    "print(f1_score(test_set_y, pred_real, average='macro'))\n",
    "print(f1_score(test_set_y, pred_fake, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copulas.univariate.KDEUnivariate import KDEUnivariate\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/iris.data.csv')\n",
    "feature1 = data['feature_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'kde' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-136230871227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mku\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mKDEUnivariate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mku\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Research/Copulas/copulas/univariate/KDEUnivariate.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, column)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mkde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKDEUnivariate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'kde' referenced before assignment"
     ]
    }
   ],
   "source": [
    "ku =KDEUnivariate()\n",
    "ku.fit(feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
